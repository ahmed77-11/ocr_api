{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection System\n",
    "## Draft Document Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:54:37.655419Z",
     "start_time": "2025-05-26T21:54:37.642320Z"
    }
   },
   "source": "",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:54:37.671415Z",
     "start_time": "2025-05-26T21:54:37.665936Z"
    }
   },
   "source": "",
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:54:40.818159Z",
     "start_time": "2025-05-26T21:54:37.674963Z"
    }
   },
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# %% [markdown]\n",
    "# # Fraud Detection System\n",
    "# Complete implementation with feature balancing and threshold adjustments\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Initial Setup\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Constants\n",
    "AMOUNT_HIGH_QUANTILE = 0.95\n",
    "GAP_LONG_DAYS = 90\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Data Loading & Validation\n",
    "\n",
    "# %%\n",
    "def load_and_validate_data():\n",
    "    df = pd.read_sql('SELECT * FROM drafts', 'sqlite:///drafts.db')\n",
    "    \n",
    "    required_cols = ['amount_digits', 'date_created', 'date_due', 'rib']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    # Type conversions and cleaning\n",
    "    df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
    "    df['date_due'] = pd.to_datetime(df['date_due'], errors='coerce')\n",
    "    df['amount_digits'] = pd.to_numeric(df['amount_digits'], errors='coerce')\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    df = df.dropna(subset=required_cols)\n",
    "    print(f\"Removed {initial_count - len(df)} invalid records\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Feature Engineering\n",
    "\n",
    "# %%\n",
    "def create_features(df):\n",
    "    # Time-based features\n",
    "    df['gap_days'] = (df['date_due'] - df['date_created']).dt.days\n",
    "    df['gap_negative'] = (df['gap_days'] < 0).astype(int)\n",
    "    df['gap_long'] = (df['gap_days'] > GAP_LONG_DAYS).astype(int)\n",
    "    \n",
    "    # Amount features\n",
    "    amount_threshold = df['amount_digits'].quantile(AMOUNT_HIGH_QUANTILE)\n",
    "    df['amount_high'] = (df['amount_digits'] > amount_threshold).astype(int)\n",
    "    df['amount_log'] = np.log1p(df['amount_digits'])\n",
    "    joblib.dump(amount_threshold, 'amount_threshold.pkl')\n",
    "    \n",
    "    # Validation features\n",
    "    df['amount_words_match'] = df.apply(\n",
    "        lambda x: fuzzy_match_amount_words(x['amount_digits'], x.get('amount_words', '')),\n",
    "        axis=1\n",
    "    )\n",
    "    df['sig_missing'] = (~df['signature_detected']).astype(int) if 'signature_detected' in df.columns else 0\n",
    "    df['barcode_bad'] = (~df['barcode_validates_traite']).astype(int) if 'barcode_validates_traite' in df.columns else 0\n",
    "    df['rib_invalid'] = df['rib'].apply(lambda x: not is_valid_rib(x))\n",
    "    \n",
    "    # Text features\n",
    "    text_cols = ['payer_name_address', 'drawer_name']\n",
    "    for col in text_cols:\n",
    "        df[f'{col}_len'] = df[col].str.len() if col in df.columns else 30\n",
    "    \n",
    "    return df\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Helper Functions\n",
    "\n",
    "# %%\n",
    "def is_valid_rib(rib):\n",
    "    try:\n",
    "        rib_clean = ''.join(filter(str.isdigit, str(rib)))\n",
    "        if len(rib_clean) != 20: return False\n",
    "        if rib_clean.startswith('000'): return False\n",
    "        return int(rib_clean[18:20]) > 0  # Basic check digit simulation\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_amount_words(amount, words):\n",
    "    try:\n",
    "        from num2words import num2words\n",
    "        amount = float(amount)\n",
    "        words = str(words).lower()\n",
    "        \n",
    "        if str(int(amount)) in words:\n",
    "            return True\n",
    "            \n",
    "        expected = num2words(amount, lang='fr').replace('-', ' ')\n",
    "        return any(word in expected for word in words.split())\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Model Training\n",
    "\n",
    "# %%\n",
    "def train_models(X_train, y_train):\n",
    "    # Class weighting\n",
    "    classes = np.unique(y_train)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    class_weights = dict(zip(classes, weights))\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    numeric_features = ['amount_digits', 'gap_days', 'amount_log']\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_features = ['amount_high', 'gap_long', 'gap_negative',\n",
    "                           'amount_words_match', 'sig_missing', 'barcode_bad',\n",
    "                           'rib_invalid']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', 'passthrough', categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Initialize models\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        class_weight='balanced_subsample',\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    lr = LogisticRegression(\n",
    "        class_weight=class_weights,\n",
    "        penalty='l2',\n",
    "        C=0.1,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    nn = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(16, activation='relu', kernel_regularizer='l2'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    nn.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='pr_auc', curve='PR')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'preprocessor': preprocessor,\n",
    "        'rf': rf,\n",
    "        'lr': lr,\n",
    "        'nn': nn,\n",
    "        'class_weights': class_weights\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Prediction System\n",
    "\n",
    "# %%\n",
    "def predict_fraud(input_data, model, preprocessor, threshold=0.5):\n",
    "    # Feature engineering\n",
    "    features = pd.DataFrame([{\n",
    "        'amount_digits': input_data['amount_digits'],\n",
    "        'gap_days': (pd.to_datetime(input_data['date_due']) - \n",
    "                    pd.to_datetime(input_data['date_created'])).days,\n",
    "        'amount_log': np.log1p(input_data['amount_digits']),\n",
    "        'amount_high': int(input_data['amount_digits'] > joblib.load('amount_threshold.pkl')),\n",
    "        'gap_long': int((pd.to_datetime(input_data['date_due']) - \n",
    "                       pd.to_datetime(input_data['date_created'])).days > GAP_LONG_DAYS),\n",
    "        'gap_negative': int(pd.to_datetime(input_data['date_due']) < \n",
    "                        pd.to_datetime(input_data['date_created'])),\n",
    "        'amount_words_match': fuzzy_match_amount_words(input_data['amount_digits'], \n",
    "                                                      input_data.get('amount_words', '')),\n",
    "        'sig_missing': int(not input_data.get('signature_detected', False)),\n",
    "        'barcode_bad': int(not input_data.get('barcode_validates_traite', False)),\n",
    "        'rib_invalid': int(not is_valid_rib(input_data.get('rib', '')))\n",
    "    }])\n",
    "    \n",
    "    # Preprocessing and prediction\n",
    "    X_processed = preprocessor.transform(features)\n",
    "    \n",
    "    if isinstance(model, tf.keras.Model):\n",
    "        prob = model.predict(X_processed)[0][0]\n",
    "    else:\n",
    "        prob = model.predict_proba(X_processed)[0][1]\n",
    "    \n",
    "    # Conflict resolution\n",
    "    conflict_score = 0\n",
    "    if features['amount_words_match'][0] and not features['sig_missing'][0]:\n",
    "        conflict_score -= 0.3\n",
    "    if features['rib_invalid'][0] and features['barcode_bad'][0]:\n",
    "        conflict_score += 0.4\n",
    "    \n",
    "    final_prob = np.clip(prob + conflict_score, 0, 1)\n",
    "    \n",
    "    return {\n",
    "        'fraud_probability': final_prob,\n",
    "        'prediction': final_prob > threshold,\n",
    "        'confidence': 'High' if abs(final_prob - 0.5) > 0.3 else 'Medium',\n",
    "        'feature_values': features.to_dict(),\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Execution & Testing\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    # Data pipeline\n",
    "    df = load_and_validate_data()\n",
    "    df = create_features(df)\n",
    "    \n",
    "    # Prepare features\n",
    "    features = ['amount_digits', 'gap_days', 'amount_log', 'amount_high',\n",
    "               'gap_long', 'gap_negative', 'amount_words_match', 'sig_missing',\n",
    "               'barcode_bad', 'rib_invalid']\n",
    "    X = df[features]\n",
    "    y = df['fraud_label'] if 'fraud_label' in df.columns else np.zeros(len(df))\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Model training\n",
    "    models = train_models(X_train, y_train)\n",
    "    preprocessor = models['preprocessor']\n",
    "    \n",
    "    # Fit preprocessing\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Train classifiers\n",
    "    models['rf'].fit(X_train_processed, y_train)\n",
    "    models['lr'].fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Neural network training\n",
    "    models['nn'].fit(\n",
    "        X_train_processed, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        class_weight=models['class_weights'],\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        ],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Save models\n",
    "    joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "    joblib.dump(models['rf'], 'rf_model.pkl')\n",
    "    joblib.dump(models['lr'], 'lr_model.pkl')\n",
    "    models['nn'].save('nn_model.keras')"
   ],
   "execution_count": 4,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
