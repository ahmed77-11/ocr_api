{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Detection System\n",
    "## Draft Document Analysis Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:54:37.655419Z",
     "start_time": "2025-05-26T21:54:37.642320Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:54:37.671415Z",
     "start_time": "2025-05-26T21:54:37.665936Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-26T21:54:40.818159Z",
     "start_time": "2025-05-26T21:54:37.674963Z"
    }
   },
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# %% [markdown]\n",
    "# # Fraud Detection System\n",
    "# Complete implementation with feature balancing and threshold adjustments\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Initial Setup\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score, precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# Constants\n",
    "AMOUNT_HIGH_QUANTILE = 0.95\n",
    "GAP_LONG_DAYS = 90\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Data Loading & Validation\n",
    "\n",
    "# %%\n",
    "def load_and_validate_data():\n",
    "    df = pd.read_sql('SELECT * FROM drafts', 'sqlite:///drafts.db')\n",
    "    \n",
    "    required_cols = ['amount_digits', 'date_created', 'date_due', 'rib']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Missing required column: {col}\")\n",
    "    \n",
    "    # Type conversions and cleaning\n",
    "    df['date_created'] = pd.to_datetime(df['date_created'], errors='coerce')\n",
    "    df['date_due'] = pd.to_datetime(df['date_due'], errors='coerce')\n",
    "    df['amount_digits'] = pd.to_numeric(df['amount_digits'], errors='coerce')\n",
    "    \n",
    "    initial_count = len(df)\n",
    "    df = df.dropna(subset=required_cols)\n",
    "    print(f\"Removed {initial_count - len(df)} invalid records\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Feature Engineering\n",
    "\n",
    "# %%\n",
    "def create_features(df):\n",
    "    # Time-based features\n",
    "    df['gap_days'] = (df['date_due'] - df['date_created']).dt.days\n",
    "    df['gap_negative'] = (df['gap_days'] < 0).astype(int)\n",
    "    df['gap_long'] = (df['gap_days'] > GAP_LONG_DAYS).astype(int)\n",
    "    \n",
    "    # Amount features\n",
    "    amount_threshold = df['amount_digits'].quantile(AMOUNT_HIGH_QUANTILE)\n",
    "    df['amount_high'] = (df['amount_digits'] > amount_threshold).astype(int)\n",
    "    df['amount_log'] = np.log1p(df['amount_digits'])\n",
    "    joblib.dump(amount_threshold, 'amount_threshold.pkl')\n",
    "    \n",
    "    # Validation features\n",
    "    df['amount_words_match'] = df.apply(\n",
    "        lambda x: fuzzy_match_amount_words(x['amount_digits'], x.get('amount_words', '')),\n",
    "        axis=1\n",
    "    )\n",
    "    df['sig_missing'] = (~df['signature_detected']).astype(int) if 'signature_detected' in df.columns else 0\n",
    "    df['barcode_bad'] = (~df['barcode_validates_traite']).astype(int) if 'barcode_validates_traite' in df.columns else 0\n",
    "    df['rib_invalid'] = df['rib'].apply(lambda x: not is_valid_rib(x))\n",
    "    \n",
    "    # Text features\n",
    "    text_cols = ['payer_name_address', 'drawer_name']\n",
    "    for col in text_cols:\n",
    "        df[f'{col}_len'] = df[col].str.len() if col in df.columns else 30\n",
    "    \n",
    "    return df\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Helper Functions\n",
    "\n",
    "# %%\n",
    "def is_valid_rib(rib):\n",
    "    try:\n",
    "        rib_clean = ''.join(filter(str.isdigit, str(rib)))\n",
    "        if len(rib_clean) != 20: return False\n",
    "        if rib_clean.startswith('000'): return False\n",
    "        return int(rib_clean[18:20]) > 0  # Basic check digit simulation\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def fuzzy_match_amount_words(amount, words):\n",
    "    try:\n",
    "        from num2words import num2words\n",
    "        amount = float(amount)\n",
    "        words = str(words).lower()\n",
    "        \n",
    "        if str(int(amount)) in words:\n",
    "            return True\n",
    "            \n",
    "        expected = num2words(amount, lang='fr').replace('-', ' ')\n",
    "        return any(word in expected for word in words.split())\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Model Training\n",
    "\n",
    "# %%\n",
    "def train_models(X_train, y_train):\n",
    "    # Class weighting\n",
    "    classes = np.unique(y_train)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "    class_weights = dict(zip(classes, weights))\n",
    "    \n",
    "    # Preprocessing pipeline\n",
    "    numeric_features = ['amount_digits', 'gap_days', 'amount_log']\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_features = ['amount_high', 'gap_long', 'gap_negative',\n",
    "                           'amount_words_match', 'sig_missing', 'barcode_bad',\n",
    "                           'rib_invalid']\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numeric_features),\n",
    "            ('cat', 'passthrough', categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Initialize models\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        class_weight='balanced_subsample',\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    lr = LogisticRegression(\n",
    "        class_weight=class_weights,\n",
    "        penalty='l2',\n",
    "        C=0.1,\n",
    "        max_iter=1000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    nn = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(16, activation='relu', kernel_regularizer='l2'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(8, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    nn.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            tf.keras.metrics.Precision(name='precision'),\n",
    "            tf.keras.metrics.Recall(name='recall'),\n",
    "            tf.keras.metrics.AUC(name='pr_auc', curve='PR')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'preprocessor': preprocessor,\n",
    "        'rf': rf,\n",
    "        'lr': lr,\n",
    "        'nn': nn,\n",
    "        'class_weights': class_weights\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Prediction System\n",
    "\n",
    "# %%\n",
    "def predict_fraud(input_data, model, preprocessor, threshold=0.5):\n",
    "    # Feature engineering\n",
    "    features = pd.DataFrame([{\n",
    "        'amount_digits': input_data['amount_digits'],\n",
    "        'gap_days': (pd.to_datetime(input_data['date_due']) - \n",
    "                    pd.to_datetime(input_data['date_created'])).days,\n",
    "        'amount_log': np.log1p(input_data['amount_digits']),\n",
    "        'amount_high': int(input_data['amount_digits'] > joblib.load('amount_threshold.pkl')),\n",
    "        'gap_long': int((pd.to_datetime(input_data['date_due']) - \n",
    "                       pd.to_datetime(input_data['date_created'])).days > GAP_LONG_DAYS),\n",
    "        'gap_negative': int(pd.to_datetime(input_data['date_due']) < \n",
    "                        pd.to_datetime(input_data['date_created'])),\n",
    "        'amount_words_match': fuzzy_match_amount_words(input_data['amount_digits'], \n",
    "                                                      input_data.get('amount_words', '')),\n",
    "        'sig_missing': int(not input_data.get('signature_detected', False)),\n",
    "        'barcode_bad': int(not input_data.get('barcode_validates_traite', False)),\n",
    "        'rib_invalid': int(not is_valid_rib(input_data.get('rib', '')))\n",
    "    }])\n",
    "    \n",
    "    # Preprocessing and prediction\n",
    "    X_processed = preprocessor.transform(features)\n",
    "    \n",
    "    if isinstance(model, tf.keras.Model):\n",
    "        prob = model.predict(X_processed)[0][0]\n",
    "    else:\n",
    "        prob = model.predict_proba(X_processed)[0][1]\n",
    "    \n",
    "    # Conflict resolution\n",
    "    conflict_score = 0\n",
    "    if features['amount_words_match'][0] and not features['sig_missing'][0]:\n",
    "        conflict_score -= 0.3\n",
    "    if features['rib_invalid'][0] and features['barcode_bad'][0]:\n",
    "        conflict_score += 0.4\n",
    "    \n",
    "    final_prob = np.clip(prob + conflict_score, 0, 1)\n",
    "    \n",
    "    return {\n",
    "        'fraud_probability': final_prob,\n",
    "        'prediction': final_prob > threshold,\n",
    "        'confidence': 'High' if abs(final_prob - 0.5) > 0.3 else 'Medium',\n",
    "        'feature_values': features.to_dict(),\n",
    "        'threshold': threshold\n",
    "    }\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Execution & Testing\n",
    "\n",
    "# %%\n",
    "if __name__ == \"__main__\":\n",
    "    # Data pipeline\n",
    "    df = load_and_validate_data()\n",
    "    df = create_features(df)\n",
    "    \n",
    "    # Prepare features\n",
    "    features = ['amount_digits', 'gap_days', 'amount_log', 'amount_high',\n",
    "               'gap_long', 'gap_negative', 'amount_words_match', 'sig_missing',\n",
    "               'barcode_bad', 'rib_invalid']\n",
    "    X = df[features]\n",
    "    y = df['fraud_label'] if 'fraud_label' in df.columns else np.zeros(len(df))\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Model training\n",
    "    models = train_models(X_train, y_train)\n",
    "    preprocessor = models['preprocessor']\n",
    "    \n",
    "    # Fit preprocessing\n",
    "    X_train_processed = preprocessor.fit_transform(X_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Train classifiers\n",
    "    models['rf'].fit(X_train_processed, y_train)\n",
    "    models['lr'].fit(X_train_processed, y_train)\n",
    "    \n",
    "    # Neural network training\n",
    "    models['nn'].fit(\n",
    "        X_train_processed, y_train,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        class_weight=models['class_weights'],\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "        ],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Save models\n",
    "    joblib.dump(preprocessor, 'preprocessor.pkl')\n",
    "    joblib.dump(models['rf'], 'rf_model.pkl')\n",
    "    joblib.dump(models['lr'], 'lr_model.pkl')\n",
    "    models['nn'].save('nn_model.keras')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 invalid records\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: object",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[4], line 263\u001B[0m\n\u001B[0;32m    260\u001B[0m models[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlr\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mfit(X_train_processed, y_train)\n\u001B[0;32m    262\u001B[0m \u001B[38;5;66;03m# Neural network training\u001B[39;00m\n\u001B[1;32m--> 263\u001B[0m \u001B[43mmodels\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnn\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    264\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX_train_processed\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    265\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    266\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m32\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    267\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_split\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    268\u001B[0m \u001B[43m    \u001B[49m\u001B[43mclass_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodels\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mclass_weights\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    269\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\n\u001B[0;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEarlyStopping\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrestore_best_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    272\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\n\u001B[0;32m    273\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    275\u001B[0m \u001B[38;5;66;03m# Save models\u001B[39;00m\n\u001B[0;32m    276\u001B[0m joblib\u001B[38;5;241m.\u001B[39mdump(preprocessor, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpreprocessor.pkl\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[1;32m~\\Desktop\\layoutMl\\.venv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\Desktop\\layoutMl\\.venv\\lib\\site-packages\\optree\\ops.py:766\u001B[0m, in \u001B[0;36mtree_map\u001B[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001B[0m\n\u001B[0;32m    764\u001B[0m leaves, treespec \u001B[38;5;241m=\u001B[39m _C\u001B[38;5;241m.\u001B[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001B[0;32m    765\u001B[0m flat_args \u001B[38;5;241m=\u001B[39m [leaves] \u001B[38;5;241m+\u001B[39m [treespec\u001B[38;5;241m.\u001B[39mflatten_up_to(r) \u001B[38;5;28;01mfor\u001B[39;00m r \u001B[38;5;129;01min\u001B[39;00m rests]\n\u001B[1;32m--> 766\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtreespec\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munflatten\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mflat_args\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: Invalid dtype: object"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
